{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 基于CIFAR-10数据集的图片分类任务","metadata":{}},{"cell_type":"code","source":"!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2024-06-19T03:03:35.489327Z","iopub.execute_input":"2024-06-19T03:03:35.489718Z","iopub.status.idle":"2024-06-19T03:03:47.646070Z","shell.execute_reply.started":"2024-06-19T03:03:35.489686Z","shell.execute_reply":"2024-06-19T03:03:47.645053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train cifar dataset\n\nimport numpy as np\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torchsummary import summary\nimport matplotlib.pyplot as plt\nimport time\n\n# define the transform of the data\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # mean=0.5,std=0.5,data range[-1,1]\n\n# download the data\ncifar_root = '/Users/lvangge/Documents/ /code/codes_/神经网络深度学习/pj2'\ntrainset = torchvision.datasets.CIFAR10(root=cifar_root, train=True, download=True, transform=transform)\ntestset = torchvision.datasets.CIFAR10(root=cifar_root, train=False, download=True, transform=transform)\n\ntrainloader = DataLoader(trainset, batch_size=64, shuffle=2, num_workers=2)\ntestloader = DataLoader(testset, batch_size=64, shuffle=2, num_workers=2)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-19T03:03:50.561375Z","iopub.execute_input":"2024-06-19T03:03:50.562241Z","iopub.status.idle":"2024-06-19T03:03:52.176014Z","shell.execute_reply.started":"2024-06-19T03:03:50.562202Z","shell.execute_reply":"2024-06-19T03:03:52.175148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **定义训练函数和测试函数，返回损失函数数值和错误率，主要训练架构**","metadata":{}},{"cell_type":"code","source":"def train(dataloader, loss_func, optimizer, model, device):\n    size = len(dataloader.dataset)\n    num_batch = len(dataloader)\n    train_loss, train_err = 0, 0\n    \n    for x, y in dataloader:\n        x, y = x.to(device), y.to(device)\n        pred = model(x)\n        loss = loss_func(pred, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        train_err += (torch.argmax(F.softmax(pred, dim=1), dim=1) != y).type(torch.float).sum().item()\n\n    train_loss /= num_batch\n    train_err /= size\n\n    return train_loss, train_err\n\ndef test(dataloader, loss_func, optimizer, model, device=device):\n    size = len(dataloader.dataset)\n    num_batch = len(dataloader)\n    test_loss, test_err = 0, 0\n    \n    for x, y in dataloader:\n        x, y = x.to(device), y.to(device)\n        pred = model(x)\n        loss = loss_func(pred, y)\n\n        test_loss += loss.item()\n        test_err += (torch.argmax(F.softmax(pred, dim=1), dim=1) != y).type(torch.float).sum().item()\n\n    test_loss /= num_batch\n    test_err /= size\n\n    return test_loss, test_err\nprint('function done')","metadata":{"execution":{"iopub.status.busy":"2024-06-19T03:03:57.510486Z","iopub.execute_input":"2024-06-19T03:03:57.511277Z","iopub.status.idle":"2024-06-19T03:03:57.522072Z","shell.execute_reply.started":"2024-06-19T03:03:57.511241Z","shell.execute_reply":"2024-06-19T03:03:57.521101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**主要训练框架,初步训练模型**","metadata":{}},{"cell_type":"code","source":"nclass = 10\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, kernel_size=5) # 32 -> 28\n        self.pool1 = nn.MaxPool2d(kernel_size=2) # 28 -> 14\n        self.conv2 = nn.Conv2d(6, 16, kernel_size=5) # 14 -> 10\n        self.pool2 = nn.MaxPool2d(kernel_size=2) # 10 -> 5\n        self.fc1 = nn.Linear(16*5*5, 64)\n        self.dr = nn.Dropout(p=0.5)\n        self.fc2 = nn.Linear(64, nclass)\n\n    def forward(self, x):\n        x = self.pool1(F.relu(self.conv1(x)))\n        x = self.pool2(F.relu(self.conv2(x)))\n        x = torch.flatten(x, start_dim=1)\n        x = F.relu(self.fc1(x))\n        x = self.dr(x)\n        x = F.relu(self.fc2(x))\n        \n        return x\n\nmodel = Model()\nsummary(model, input_size=(3,32,32), device='cpu')\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nloss_func = nn.CrossEntropyLoss()\nlr = 1e-3\noptimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum = 0.9)\n\nif __name__ == '__main__':\n    epochs = 10\n    train_loss = []\n    train_err = []\n    test_loss = []\n    test_err =[]\n    start_time = time.time()\n\n    for epoch in range(epochs):\n        model.train()\n        e_train_loss, e_train_err = train(trainloader, loss_func, optimizer, model, device)\n\n        model.eval()\n        e_test_loss, e_test_err = test(testloader, loss_func, optimizer, model)\n\n        train_loss.append(e_test_loss)\n        train_err.append(e_train_err)\n        test_loss.append(e_train_loss)\n        test_err.append(e_test_err)\n\n        template = ('Epoch:{:2d}, train_err:{:.1f}%, train_loss:{:.3f}, test_err:{:.1f}%, test_loss:{:.3f}')\n        print(template.format(epoch+1, e_train_err*100, e_train_loss, e_test_err*100, e_test_loss))\n#         epoch_time = time.time()\n#         print(f'Epoch{epoch+1} time: {epoch_time - start_time}')\n\ndef plot():\n    epoch_range = range(epochs)\n    plt.figure(figsize=(12,3))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(epoch_range, train_loss, label='Train Loss')\n    plt.plot(epoch_range, test_loss, label='Test Loss')\n    plt.legend()\n    plt.title('Loss Plot')\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(epoch_range, train_err, label='Train Error')\n    plt.plot(epoch_range, test_err, label='Test Error')\n    plt.legend()\n    plt.title('Error Plot')\n    \n    plt.show()\n\nplot()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-19T03:03:28.924645Z","iopub.execute_input":"2024-06-19T03:03:28.925164Z","iopub.status.idle":"2024-06-19T03:03:30.987627Z","shell.execute_reply.started":"2024-06-19T03:03:28.925139Z","shell.execute_reply":"2024-06-19T03:03:30.985240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**尝试使用更复杂的模型**","metadata":{}},{"cell_type":"code","source":"nclass = 10\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=5) #inpue size 32*32\n        self.pool1 = nn.MaxPool2d(kernel_size=2) # 28\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=5) #14\n        self.pool2 = nn.MaxPool2d(kernel_size=2) #10\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=5) #size 5*5 -> 1*1\n        self.fc1 = nn.Linear(128, 64)\n        self.dr = nn.Dropout(p=0.5)\n        self.fc2 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = self.pool1(F.relu(self.conv1(x)))\n        x = self.pool2(F.relu(self.conv2(x)))\n        x = F.relu(self.conv3(x))\n        x = torch.flatten(x, start_dim=1)\n        x = F.relu(self.fc1(x))\n        x = self.dr(x)\n        x = F.relu(self.fc2(x))\n        \n        return x\n\nmodel = Model()\nsummary(model, input_size=(3,32,32), device='cpu')\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T03:04:09.962142Z","iopub.execute_input":"2024-06-19T03:04:09.962534Z","iopub.status.idle":"2024-06-19T03:04:09.989424Z","shell.execute_reply.started":"2024-06-19T03:04:09.962502Z","shell.execute_reply":"2024-06-19T03:04:09.988521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_func = nn.CrossEntropyLoss()\nlr = 1e-3\noptimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum = 0.9)\n\nif __name__ == '__main__':\n    epochs = 10\n    train_loss = []\n    train_err = []\n    test_loss = []\n    test_err =[]\n    start_time = time.time()\n\n    for epoch in range(epochs):\n        model.train()\n        e_train_loss, e_train_err = train(trainloader, loss_func, optimizer, model, device)\n\n        model.eval()\n        e_test_loss, e_test_err = test(testloader, loss_func, optimizer, model)\n\n        train_loss.append(e_test_loss)\n        train_err.append(e_train_err)\n        test_loss.append(e_train_loss)\n        test_err.append(e_test_err)\n\n        template = ('Epoch:{:2d}, train_err:{:.1f}%, train_loss:{:.3f}, test_err:{:.1f}%, test_loss:{:.3f}')\n        print(template.format(epoch+1, e_train_err*100, e_train_loss, e_test_err*100, e_test_loss))\n#         epoch_time = time.time()\n#         print(f'Epoch{epoch+1} time: {epoch_time - start_time}')\n\ndef plot():\n    epoch_range = range(epochs)\n    plt.figure(figsize=(12,3))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(epoch_range, train_loss, label='Train Loss')\n    plt.plot(epoch_range, test_loss, label='Test Loss')\n    plt.ylim(0,3)\n    plt.legend()\n    plt.title('Loss Plot')\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(epoch_range, train_err, label='Train Error')\n    plt.plot(epoch_range, test_err, label='Test Error')\n    plt.ylim(0,1)\n    plt.legend()\n    plt.title('Error Plot')\n    \n    plt.show()\n\nplot()","metadata":{"execution":{"iopub.status.busy":"2024-06-19T03:03:30.990300Z","iopub.status.idle":"2024-06-19T03:03:30.990627Z","shell.execute_reply.started":"2024-06-19T03:03:30.990471Z","shell.execute_reply":"2024-06-19T03:03:30.990484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**更换复杂的模型后大大提升了网络中神经元的数量，但是对训练结果的改进却很小，下面尝试使用其他的优化器，e.g. Adam**","metadata":{}},{"cell_type":"code","source":"nclass = 10\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, kernel_size=5) # 32 -> 28\n        self.pool1 = nn.MaxPool2d(kernel_size=2) # 28 -> 14\n        self.conv2 = nn.Conv2d(6, 16, kernel_size=5) # 14 -> 10\n        self.pool2 = nn.MaxPool2d(kernel_size=2) # 10 -> 5\n        self.fc1 = nn.Linear(16*5*5, 64)\n        self.dr = nn.Dropout(p=0.5)\n        self.fc2 = nn.Linear(64, nclass)\n\n    def forward(self, x):\n        x = self.pool1(F.relu(self.conv1(x)))\n        x = self.pool2(F.relu(self.conv2(x)))\n        x = torch.flatten(x, start_dim=1)\n        x = F.relu(self.fc1(x))\n        x = self.dr(x)\n        x = F.relu(self.fc2(x))\n        \n        return x\n\nmodel = Model()\nsummary(model, input_size=(3,32,32), device='cpu')\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-19T03:04:20.675393Z","iopub.execute_input":"2024-06-19T03:04:20.676250Z","iopub.status.idle":"2024-06-19T03:04:20.694899Z","shell.execute_reply.started":"2024-06-19T03:04:20.676217Z","shell.execute_reply":"2024-06-19T03:04:20.694072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_func = nn.CrossEntropyLoss()\nlr = 1e-3\noptimizer = torch.optim.Adam(model.parameters(),lr = lr)\n\nif __name__ == '__main__':\n    epochs = 50\n    train_loss = []\n    train_err = []\n    test_loss = []\n    test_err =[]\n    start_time = time.time()\n\n    for epoch in range(epochs):\n        model.train()\n        e_train_loss, e_train_err = train(trainloader, loss_func, optimizer, model, device)\n\n        model.eval()\n        e_test_loss, e_test_err = test(testloader, loss_func, optimizer, model)\n\n        train_loss.append(e_test_loss)\n        train_err.append(e_train_err)\n        test_loss.append(e_train_loss)\n        test_err.append(e_test_err)\n\n        template = ('Epoch:{:2d}, train_err:{:.1f}%, train_loss:{:.3f}, test_err:{:.1f}%, test_loss:{:.3f}')\n        if (epoch+1) % 5 == 0:\n            print(template.format(epoch+1, e_train_err*100, e_train_loss, e_test_err*100, e_test_loss))\n#         epoch_time = time.time()\n#         print(f'Epoch{epoch+1} time: {epoch_time - start_time}')\n\ndef plot():\n    epoch_range = range(epochs)\n    plt.figure(figsize=(12,3))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(epoch_range, train_loss, label='Train Loss')\n    plt.plot(epoch_range, test_loss, label='Test Loss')\n    plt.ylim(0,3)\n    plt.legend()\n    plt.title('Loss Plot')\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(epoch_range, train_err, label='Train Error')\n    plt.plot(epoch_range, test_err, label='Test Error')\n    plt.ylim(0,1)\n    plt.legend()\n    plt.title('Error Plot')\n    \n    plt.show()\n\nplot()","metadata":{"execution":{"iopub.status.busy":"2024-06-19T03:03:30.993662Z","iopub.status.idle":"2024-06-19T03:03:30.993963Z","shell.execute_reply.started":"2024-06-19T03:03:30.993813Z","shell.execute_reply":"2024-06-19T03:03:30.993826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Adam优化器在第一个epoch表现良好，但是后续收敛速度减慢，考虑调整学习率为0.01**","metadata":{}},{"cell_type":"code","source":"loss_func = nn.CrossEntropyLoss()\nlr = 1e-2\noptimizer = torch.optim.Adam(model.parameters(),lr = lr)\n\nif __name__ == '__main__':\n    epochs = 50\n    train_loss = []\n    train_err = []\n    test_loss = []\n    test_err =[]\n    start_time = time.time()\n\n    for epoch in range(epochs):\n        model.train()\n        e_train_loss, e_train_err = train(trainloader, loss_func, optimizer, model, device)\n\n        model.eval()\n        e_test_loss, e_test_err = test(testloader, loss_func, optimizer, model)\n\n        train_loss.append(e_test_loss)\n        train_err.append(e_train_err)\n        test_loss.append(e_train_loss)\n        test_err.append(e_test_err)\n\n        template = ('Epoch:{:2d}, train_err:{:.1f}%, train_loss:{:.3f}, test_err:{:.1f}%, test_loss:{:.3f}')\n        print(template.format(epoch+1, e_train_err*100, e_train_loss, e_test_err*100, e_test_loss))\n#         epoch_time = time.time()\n#         print(f'Epoch{epoch+1} time: {epoch_time - start_time}')\n\ndef plot():\n    epoch_range = range(epochs)\n    plt.figure(figsize=(12,3))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(epoch_range, train_loss, label='Train Loss')\n    plt.plot(epoch_range, test_loss, label='Test Loss')\n    plt.ylim(0,3)\n    plt.legend()\n    plt.title('Loss Plot')\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(epoch_range, train_err, label='Train Error')\n    plt.plot(epoch_range, test_err, label='Test Error')\n    plt.ylim(0,1)\n    plt.legend()\n    plt.title('Error Plot')\n    \n    plt.show()\n\nplot()","metadata":{"execution":{"iopub.status.busy":"2024-06-19T03:03:30.995328Z","iopub.status.idle":"2024-06-19T03:03:30.995645Z","shell.execute_reply.started":"2024-06-19T03:03:30.995479Z","shell.execute_reply":"2024-06-19T03:03:30.995492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 观察可知此时训练结果出现震荡，考虑采用学习率波动衰减策略","metadata":{}},{"cell_type":"markdown","source":"使用CosineAnnealingLR","metadata":{"execution":{"iopub.status.busy":"2024-06-19T02:39:11.379850Z","iopub.status.idle":"2024-06-19T02:39:11.380196Z","shell.execute_reply.started":"2024-06-19T02:39:11.379997Z","shell.execute_reply":"2024-06-19T02:39:11.380010Z"}}},{"cell_type":"code","source":"from torch.optim.lr_scheduler import CosineAnnealingLR","metadata":{"execution":{"iopub.status.busy":"2024-06-19T03:04:26.880376Z","iopub.execute_input":"2024-06-19T03:04:26.881091Z","iopub.status.idle":"2024-06-19T03:04:26.885275Z","shell.execute_reply.started":"2024-06-19T03:04:26.881057Z","shell.execute_reply":"2024-06-19T03:04:26.884315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(dataloader, loss_func, optimizer, model, device):\n    size = len(dataloader.dataset)\n    num_batch = len(dataloader)\n    train_loss, train_err = 0, 0\n    \n    for x, y in dataloader:\n        x, y = x.to(device), y.to(device)\n        pred = model(x)\n        loss = loss_func(pred, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        train_err += (torch.argmax(F.softmax(pred, dim=1), dim=1) != y).type(torch.float).sum().item()\n\n    train_loss /= num_batch\n    train_err /= size\n\n    return train_loss, train_err\n","metadata":{"execution":{"iopub.status.busy":"2024-06-19T03:03:30.998264Z","iopub.status.idle":"2024-06-19T03:03:30.998572Z","shell.execute_reply.started":"2024-06-19T03:03:30.998416Z","shell.execute_reply":"2024-06-19T03:03:30.998428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model()\nsummary(model, input_size=(3,32,32), device='cpu')\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nlr = 1e-3\nloss_func = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\nlr_schedule = CosineAnnealingLR(optimizer, T_max = 25)\n\nif __name__ == '__main__':\n    epochs = 50\n    train_loss = []\n    train_err = []\n    test_loss = []\n    test_err =[]\n    start_time = time.time()\n\n    for epoch in range(epochs):\n        model.train()\n        e_train_loss, e_train_err = train(trainloader, loss_func, optimizer, model, device)\n        lr_schedule.step()\n\n        model.eval()\n        e_test_loss, e_test_err = test(testloader, loss_func, optimizer, model)\n\n        train_loss.append(e_test_loss)\n        train_err.append(e_train_err)\n        test_loss.append(e_train_loss)\n        test_err.append(e_test_err)\n\n        template = ('Epoch:{:2d}, train_err:{:.1f}%, train_loss:{:.3f}, test_err:{:.1f}%, test_loss:{:.3f}')\n        if (epoch+1) % 5 == 0:\n            print(template.format(epoch+1, e_train_err*100, e_train_loss, e_test_err*100, e_test_loss))\n#         epoch_time = time.time()\n#         print(f'Epoch{epoch+1} time: {epoch_time - start_time}')\n\ndef plot():\n    epoch_range = range(epochs)\n    plt.figure(figsize=(12,3))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(epoch_range, train_loss, label='Train Loss')\n    plt.plot(epoch_range, test_loss, label='Test Loss')\n    plt.ylim(0,3)\n    plt.legend()\n    plt.title('Loss Plot')\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(epoch_range, train_err, label='Train Error')\n    plt.plot(epoch_range, test_err, label='Test Error')\n    plt.ylim(0,1)\n    plt.legend()\n    plt.title('Error Plot')\n    \n    plt.show()\n\nplot()","metadata":{"execution":{"iopub.status.busy":"2024-06-19T03:18:05.225451Z","iopub.execute_input":"2024-06-19T03:18:05.225864Z","iopub.status.idle":"2024-06-19T03:26:55.106025Z","shell.execute_reply.started":"2024-06-19T03:18:05.225829Z","shell.execute_reply":"2024-06-19T03:26:55.104998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"训练效果无明显进步，尝试配合使用复杂模型和Adam优化器","metadata":{}},{"cell_type":"code","source":"nclass = 10\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=5) #inpue size 32 -> 28\n        self.pool1 = nn.MaxPool2d(kernel_size=2) # 28 -> 14\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=5) #14 ->10 \n        self.pool2 = nn.MaxPool2d(kernel_size=2) #10 -> 5\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3) #5 -> 3\n        self.fc1 = nn.Linear(128*3*3, 64)\n        self.dr = nn.Dropout(p=0.5)\n        self.fc2 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = self.pool1(F.relu(self.conv1(x)))\n        x = self.pool2(F.relu(self.conv2(x)))\n        x = F.relu(self.conv3(x))\n        x = torch.flatten(x, start_dim=1)\n        x = F.relu(self.fc1(x))\n        x = self.dr(x)\n        x = F.relu(self.fc2(x))\n        \n        return x\n\nmodel = Model()\nsummary(model, input_size=(3,32,32), device='cpu')\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nloss_func = nn.CrossEntropyLoss()\nlr = 1e-3\noptimizer = torch.optim.Adam(model.parameters(),lr = lr)\n\nif __name__ == '__main__':\n    epochs = 50\n    train_loss = []\n    train_err = []\n    test_loss = []\n    test_err =[]\n    start_time = time.time()\n\n    for epoch in range(epochs):\n        model.train()\n        e_train_loss, e_train_err = train(trainloader, loss_func, optimizer, model, device)\n\n        model.eval()\n        e_test_loss, e_test_err = test(testloader, loss_func, optimizer, model)\n\n        train_loss.append(e_train_loss)\n        train_err.append(e_train_err)\n        test_loss.append(e_test_loss)\n        test_err.append(e_test_err)\n\n        template = ('Epoch:{:2d}, train_err:{:.1f}%, train_loss:{:.3f}, test_err:{:.1f}%, test_loss:{:.3f}')\n        if (epoch+1) % 5 == 0:\n            print(template.format(epoch+1, e_train_err*100, e_train_loss, e_test_err*100, e_test_loss))\n    end_time = time.time()\n    print('runing time: {:.2f}s'.format(end_time - start_time))\n\ndef plot():\n    epoch_range = range(epochs)\n    plt.figure(figsize=(12,3))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(epoch_range, train_loss, label='Train Loss')\n    plt.plot(epoch_range, test_loss, label='Test Loss')\n    plt.ylim(0,3)\n    plt.legend()\n    plt.title('Loss Plot')\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(epoch_range, train_err, label='Train Error')\n    plt.plot(epoch_range, test_err, label='Test Error')\n    plt.ylim(0,1)\n    plt.legend()\n    plt.title('Error Plot')\n    \n    plt.show()\n\nplot()","metadata":{"execution":{"iopub.status.busy":"2024-06-19T03:03:31.001645Z","iopub.status.idle":"2024-06-19T03:03:31.001981Z","shell.execute_reply.started":"2024-06-19T03:03:31.001817Z","shell.execute_reply":"2024-06-19T03:03:31.001832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"训练效果改进明显，但是在使用复杂模型跑50个epoch后过拟合明显","metadata":{}},{"cell_type":"markdown","source":"## 尝试进行数据增广","metadata":{}},{"cell_type":"markdown","source":"使用tochvision的transform","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as transforms\n\ntrain_transform = transforms.Compose([\n    transforms.RandomVerticalFlip(p=0.5),\n    transforms.RandomRotation((-45, 45)),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n])\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n])\nprint('transform done')","metadata":{"execution":{"iopub.status.busy":"2024-06-19T03:28:00.444664Z","iopub.execute_input":"2024-06-19T03:28:00.445584Z","iopub.status.idle":"2024-06-19T03:28:00.453541Z","shell.execute_reply.started":"2024-06-19T03:28:00.445539Z","shell.execute_reply":"2024-06-19T03:28:00.452583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cifar_root = '/Users/lvangge/Documents/ /code/codes_/神经网络深度学习/pj2'\ndef load_data(root,is_train,transform):\n    set = torchvision.datasets.CIFAR10(root=cifar_root, train=is_train, download=False, transform=transform)\n    dataloader = DataLoader(set, batch_size=128, shuffle=True, num_workers=4,pin_memory=True)\n    return dataloader\nprint('dataloader done')","metadata":{"execution":{"iopub.status.busy":"2024-06-19T03:28:04.128854Z","iopub.execute_input":"2024-06-19T03:28:04.129540Z","iopub.status.idle":"2024-06-19T03:28:04.135540Z","shell.execute_reply.started":"2024-06-19T03:28:04.129507Z","shell.execute_reply":"2024-06-19T03:28:04.134640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nclass = 10\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=5) #inpue size 32 -> 28\n        self.pool1 = nn.MaxPool2d(kernel_size=2) # 28 -> 14\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=5) #14 ->10 \n        self.pool2 = nn.MaxPool2d(kernel_size=2) #10 -> 5\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3) #5 -> 3\n        self.fc1 = nn.Linear(128*3*3, 64)\n        self.dr = nn.Dropout(p=0.5)\n        self.fc2 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = self.pool1(F.relu(self.conv1(x)))\n        x = self.pool2(F.relu(self.conv2(x)))\n        x = F.relu(self.conv3(x))\n        x = torch.flatten(x, start_dim=1)\n        x = F.relu(self.fc1(x))\n        x = self.dr(x)\n        x = F.relu(self.fc2(x))\n        \n        return x\n\nmodel = Model()\nsummary(model, input_size=(3,32,32), device='cpu')\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\nmodel.to(device)\n\nloss_func = nn.CrossEntropyLoss().to(device)\nlr = 1e-3\noptimizer = torch.optim.Adam(model.parameters(),lr = lr)\n\nif __name__ == '__main__':\n    epochs = 50\n    train_loss = []\n    train_err = []\n    test_loss = []\n    test_err =[]\n    start_time = time.time()\n\n    for epoch in range(epochs):\n        trianloader = load_data(cifar_root, is_train=True, transform=train_transform)\n        model.train()\n        e_train_loss, e_train_err = train(trainloader, loss_func, optimizer, model, device=device)\n        \n        testloader = load_data(cifar_root, is_train=False, transform=test_transform)\n        model.eval()\n        e_test_loss, e_test_err = test(testloader, loss_func, optimizer, model, device=device)\n\n        train_loss.append(e_train_loss)\n        train_err.append(e_train_err)\n        test_loss.append(e_test_loss)\n        test_err.append(e_test_err)\n\n        template = ('Epoch:{:2d}, train_err:{:.1f}%, train_loss:{:.3f}, test_err:{:.1f}%, test_loss:{:.3f}')\n        if (epoch+1) % 5 == 0:\n            print(template.format(epoch+1, e_train_err*100, e_train_loss, e_test_err*100, e_test_loss))\n    end_time = time.time()\n    print('runing time: {:.2f}s'.format(end_time - start_time))\n\ndef plot():\n    epoch_range = range(epochs)\n    plt.figure(figsize=(12,3))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(epoch_range, train_loss, label='Train Loss')\n    plt.plot(epoch_range, test_loss, label='Test Loss')\n    plt.ylim(0,3)\n    plt.legend()\n    plt.title('Loss Plot')\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(epoch_range, train_err, label='Train Error')\n    plt.plot(epoch_range, test_err, label='Test Error')\n    plt.ylim(0,1)\n    plt.legend()\n    plt.title('Error Plot')\n    \n    plt.show()\n\nplot()","metadata":{"execution":{"iopub.status.busy":"2024-06-19T03:28:07.984672Z","iopub.execute_input":"2024-06-19T03:28:07.985427Z","iopub.status.idle":"2024-06-19T03:37:25.964317Z","shell.execute_reply.started":"2024-06-19T03:28:07.985394Z","shell.execute_reply":"2024-06-19T03:37:25.963307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"相比增广前，模型效果无太大变化","metadata":{}},{"cell_type":"markdown","source":"## 尝试使用ResNet-18网络","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n# from torch.optim.lr_scheduler import CosineAnnealingLR\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self,in_planes, planes, stride=1, downsample=None) -> None:\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,stride=stride, padding=1)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(planes)\n#         self.downsample = downsample\n#         self.stride = stride\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n#         if self.downsample is not None:\n#             identity = self.dowmsample(x)\n\n        out += self.shortcut(identity)\n        out = self.relu(out)\n\n        return out\n    \nprint('basic network done')","metadata":{"execution":{"iopub.status.busy":"2024-06-19T03:03:31.009027Z","iopub.status.idle":"2024-06-19T03:03:31.009462Z","shell.execute_reply.started":"2024-06-19T03:03:31.009245Z","shell.execute_reply":"2024-06-19T03:03:31.009263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10) -> None:\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu1 = nn.ReLU()\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.avg1 = nn.AdaptiveAvgPool2d((1,1))\n        self.fc1 = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n            \n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu1(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avg1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n\n        return x\nprint('ResNet done')","metadata":{"execution":{"iopub.status.busy":"2024-06-19T03:03:31.011115Z","iopub.status.idle":"2024-06-19T03:03:31.011561Z","shell.execute_reply.started":"2024-06-19T03:03:31.011327Z","shell.execute_reply":"2024-06-19T03:03:31.011346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(dataloader, loss_func, optimizer, model, device):\n    size = len(dataloader.dataset)\n    num_batch = len(dataloader)\n    train_loss, train_err = 0, 0\n    \n    for x, y in dataloader:\n        x, y = x.to(device), y.to(device)\n        pred = model(x)\n        loss = loss_func(pred, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n        train_loss += loss.item()\n        train_err += (torch.argmax(F.softmax(pred, dim=1), dim=1) != y).type(torch.float).sum().item()\n\n    train_loss /= num_batch\n    train_err /= size\n\n    return train_loss, train_err\n\ndef test(dataloader, loss_func, optimizer, model, device):\n    size = len(dataloader.dataset)\n    num_batch = len(dataloader)\n    test_loss, test_err = 0, 0\n    \n    for x, y in dataloader:\n        x, y = x.to(device), y.to(device)\n        pred = model(x)\n        loss = loss_func(pred, y)\n\n        test_loss += loss.item()\n        test_err += (torch.argmax(F.softmax(pred, dim=1), dim=1) != y).type(torch.float).sum().item()\n\n    test_loss /= num_batch\n    test_err /= size\n\n    return test_loss, test_err\nprint('function done')","metadata":{"execution":{"iopub.status.busy":"2024-06-19T03:03:31.015839Z","iopub.status.idle":"2024-06-19T03:03:31.016203Z","shell.execute_reply.started":"2024-06-19T03:03:31.016015Z","shell.execute_reply":"2024-06-19T03:03:31.016030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ResNet(BasicBlock, [2, 2, 2, 2])\nsummary(model, input_size=(3,32,32), device='cpu')\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\nmodel.to(device)\n\nloss_func = nn.CrossEntropyLoss().to(device)\noptimizer = torch.optim.Adam(model.parameters(),lr = 1e-3, weight_decay=1e-4)\n# lr_schedule = CosineAnnealingLR(optimizer, T_max = 25, eta_min=1e-5)\n\nif __name__ == '__main__':\n    epochs = 50\n    train_loss = []\n    train_err = []\n    test_loss = []\n    test_err =[]\n    start_time = time.time()\n\n    for epoch in range(epochs):\n        trianloader = load_data(cifar_root, is_train=True, transform=trian_transform)\n        model.train()\n        e_train_loss, e_train_err = train(trainloader, loss_func, optimizer, model, device=device)\n#         lr_schedule.step()\n        \n        testloader = load_data(cifar_root, is_train=False, transform=test_transform)\n        model.eval()\n        e_test_loss, e_test_err = test(testloader, loss_func, optimizer, model, device=device)\n\n        train_loss.append(e_train_loss)\n        train_err.append(e_train_err)\n        test_loss.append(e_test_loss)\n        test_err.append(e_test_err)\n\n        template = ('Epoch:{:2d}, train_err:{:.1f}%, train_loss:{:.3f}, test_err:{:.1f}%, test_loss:{:.3f}')\n        if (epoch+1) % 5 == 0:\n            print(template.format(epoch+1, e_train_err*100, e_train_loss, e_test_err*100, e_test_loss))\n    end_time = time.time()\n    print('runing time: {:.2f}s'.format(end_time - start_time))\n\ndef plot():\n    epoch_range = range(epochs)\n    plt.figure(figsize=(12,3))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(epoch_range, train_loss, label='Train Loss')\n    plt.plot(epoch_range, test_loss, label='Test Loss')\n    plt.ylim(0,3)\n    plt.legend()\n    plt.title('Loss Plot')\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(epoch_range, train_err, label='Train Error')\n    plt.plot(epoch_range, test_err, label='Test Error')\n    plt.ylim(0,1)\n    plt.legend()\n    plt.title('Error Plot')\n    \n    plt.show()\n\nplot()","metadata":{"execution":{"iopub.status.busy":"2024-06-19T03:03:31.017841Z","iopub.status.idle":"2024-06-19T03:03:31.018234Z","shell.execute_reply.started":"2024-06-19T03:03:31.018018Z","shell.execute_reply":"2024-06-19T03:03:31.018051Z"},"trusted":true},"execution_count":null,"outputs":[]}]}